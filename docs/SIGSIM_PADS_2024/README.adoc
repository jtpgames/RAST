= Improved RAST Simulator and Load Tester Components (PADS 2024)
:toc:
:toc-title: pass:[<h3>Table of Contents</h3>]
:toclevels: 3

:xrefstyle: short

**Last change from: 04.10.2024**

**TODO: Accepted for publication at PADS 2024/TOMACS. Add link.**

In our paper, we explain, implement and evaluate improvements to the following components of RAST:

* Predictive Model Creator,
* Simulator,
* Load Tester.

For our evaluation, we use the https://github.com/DescartesResearch/TeaStore[TeaStore benchmarking application].
We created a GitHub fork <<teastore_fork>> and modified some parameters of their logging framework to create consistent logs especially for higher intensity workloads.

Additional experimental results not published in our paper, including the Python scripts we use for similarity calculation and the validation and prediction data acquired in our experiments, can be accessed in our Datalore notebook <<datalore_notebook>>. It includes instructions, our measurements and the Python code we use for similarity calculation. To access the datalore notebook (similar to a Jupyter notebook) creation of a free account is required.

== Terminology (refer to our paper for details)

=== Training Data
Training data refers to an SQLite database containing predictor and outcome variables extracted from request logs provided by the System Under Evaluation (in our case, TeaStore).

The training data is acquired by running load tests against TeaStore, starting with low load intensity and increasing to high load intensity. The resulting Kieker log file is transformed and stored into an SQLite database, which is then used as training data for predictive modeling. The outcome is a predictive model used by the Simulator.

=== Prediction Data
Prediction data refers to a series of log files produced by the Simulator.

The prediction data is acquired by running load tests against the Simulator for each load intensity profile. Between each load test, we copy the resulting `teastore_simulation.log` file and rename it accordingly. After acquiring a log file for each load intensity profile, we use our `ResultComparer` tool found in our Datalore notebook <<datalore_notebook>>. We recommend referring to our Datalore notebook or the snapshot archive in the Artefact Submission folder for the recommended naming and structure.

=== Validation Data
Validation data refers to a series of SQLite databases created by extracting request logs from TeaStore.

The validation data we acquired is available in our Datalore notebook <<datalore_notebook>> and in `docs/SIGSIM_PADS_2024/Artefact Submission/RAST_TeaStore_Simulation_Similarity.zip`.

Acquiring the validation data from TeaStore is a more complex process involving downloading Kieker logs, transforming them, and storing them in an SQLite database. Detailed instructions for this process can be found https://github.com/jtpgames/RAST/blob/main/docs/TeaStore/ETL.adoc[here].

The validation data is acquired similarly to the prediction data: running a load test, creating a database, and repeating for each load intensity profile. Again, we recommend referring to our Datalore notebook or the snapshot archive for the recommended structure.

== Artefact Submission Inventory

This describes the contents of the folder `docs/SIGSIM_PADS_2024/Artefact Submission`.

* RAST_TeaStore_Simulation_Similarity.zip: Exported Datalore notebook snapshot 21.05.2024. The folder `TeaStoreResultComparisonData` includes both the Validation Data and Prediction Data we used in our paper (the datalore notebook contains a greater set of Prediction Data for models we did not mention in our paper).
* similarity_scores.csv: File created from the ResultComparer Python script found in our Datalore notebook.
* similarity_scores.ods: File created from the similarity_scores.csv file using LibreOffice. Includes all formulas to assess the experimental results as well as the figures found in the paper. Also includes results and figures not found in the paper.
* Figures: Includes all figures generated using the similarity_scores.ods file and published in the paper.
* Tables: Includes tables published in the paper relevant for reproduction.

== Article Claims
The article has the following claims:

* C1: RAST is able to produce ridge and decision tree regression models for TeaStore with very high R² scores (> 0.9)
* C2: Regardless of the model and correction value used, the similarity values consistently decrease as the load intensity increases.
* C3: Performing the correction loop in the Simulator leads to lower similarity values.
* C4: The ridge regression model consistently outperforms the decision tree model in terms of similarity values across all request types.
* C5: The Load Tester is able to consistently generate the same workload, i.e., generate a consistent count and sequence of requests in every execution of the load test

To verify the claims, perform the full experiment according to the <<_instructions_for_conducting_experiments>>.

The mapping between claims, figures and tables are as follows (original figures and tables are located in `docs/SIGSIM_PADS_2024/Artefact Submission`:

|===
|Claim |Figures / Tables |Additional information

|C1
|Table 3
|The predictive models are located in the folder `Automations/Training_Data/Predictive_Models` The R² score is part of the folder name of the created model, e.g., DT_**r2-0.966**_24-05-2024_16:04:06

|C2
| Fig. 8-11
| Observe the downward trend in the figures.

|C3
| Fig. 8, 10 or Fig. 9, 11
| Compare the respective figure pairs.

|C4
| Fig. 8, 9 or Fig. 10, 11, and Tables 4, 5
| Compare the respective figure pairs and tables.

|C5
|Table 2
| Look in the `Automations/Similarity_Comparison/requests_count.txt` file containing the number of requests found in the prediction and validation data.

|===

[#_instructions_for_conducting_experiments]
== Instructions for Conducting Experiments

To conduct the experiments outlined in this paper, please follow the step-by-step instructions below:

=== System Requirements

The hardware/software configuration used by the authors is:

* CPU: Intel® Core™ i5-1135G7 @ 2.40GHz × 8
* RAM: 16GB
* OS: Ubuntu 22.04.4 LTS (Jammy Jellyfish) 64-bit

Packages to install (use `sudo apt-get install X` where `X` is the name of the following packages):

* curl
* git
* docker-compose (we used version 1.29.2)
* docker (we used version 26.1.4)
* python3.10-venv
* python3.10-dev
* openjdk-11-jre-headless
* maven (we used version 3.6.3)
* screen

=== System settings

To allow establishing tens of thousands of TCP connections during a load test, we adjust some settings of the operating system. In the following, the settings are shown for the Ubuntu OS.

==== Limits
The following settings allow the operating system to open a greater amount of so-called file descriptors. File descriptors are required to open files or network sockets.

* Add a new line in */etc/pam.d/common-session*: `session required pam_limits.so`
* Add a new line in */etc/security/limits.conf*: `<username> soft nofile 50000`

Our limits are:
----
~$ ulimit -Sn
50000
~$ ulimit -Hn
1048576

----

==== TCP settings

* Add the following lines in /etc/sysctl.conf:
[source]
----
# Default settings:
# sudo sysctl -a | grep net.core.som
#   net.core.somaxconn = 4096
# sudo sysctl -a | grep netdev_max
#   net.core.netdev_max_backlog = 1000
# sudo sysctl -a | grep tcp_max_syn
#   net.ipv4.tcp_max_syn_backlog = 512
# sudo sysctl -a | grep tcp_tw_reuse
#  net.ipv4.tcp_tw_reuse = 2
# sudo sysctl -a | grep tcp_fin_timeout
#   net.ipv4.tcp_fin_timeout = 60
# sudo sysctl -a | grep local_port_range
#   net.ipv4.ip_local_port_range = 32768    60999

# Source: https://www.digitalocean.com/community/questions/max-number-of-concurrent-tcp-connections-to-droplet

net.ipv4.ip_local_port_range = 1024 61000
net.ipv4.tcp_fin_timeout = 30
net.ipv4.tcp_tw_reuse = 1
# net.core.somaxconn = 4096
net.core.netdev_max_backlog = 2000
net.ipv4.tcp_max_syn_backlog = 2048
----

=== Workflow for acquiring data for similarity comparison.

==== Automated (Recommended)

We recommend using the tmux tool when running experiments on remote machines accessed via ssh. This allows you to maintain persistent sessions, even if the connection drops.

To install tmux on Ubuntu, run:

[source,sh]
----
sudo apt install tmux
----

Once installed, instead of running your script directly in the terminal, start a tmux session by typing:

[source,sh]
----
tmux
----

Inside the session, you can run your script as usual. If your connection is interrupted, the session and script will continue running in the background. To reconnect to the session after reconnecting via SSH, use:
[source,sh]
----
tmux attach
----

You can also detach from a session at any time without stopping your script by pressing `Ctrl-b` followed by `d`. This allows you to safely disconnect from SSH and return later without losing your work.

This section describes the necessary steps to prepare the experiment and execute it up to the point of acquiring all necessary data for similarity comparison (see <<_similarity_comparison>>).
All steps described in the following are automatically performed by typing the following in a terminal:
[source,sh]
----
cd <RAST directory>/Automations
./setup_and_create_all_data_for_similarity_comparison.sh
----

Help message:
....
This script sets up and executes an experiment with RAST using TeaStore. The execution of the experiment takes approximately 1-2 hours.
At the end of the script, the calculated similarities between TeaStore and RAST's Simulator are stored in the file: Similarity_Comparison/similarity_scores.csv

Usage: ./setup_and_create_all_data_for_similarity_comparison.sh [OPTION]

Options:
  -c, --clean-start    Remove result directories and files before starting.
                       This results in a fresh start of the experiment, ensuring no previous data interferes.
  -h, --help           Display this help message and exit.
....

At the end, the script prints all functions that it executed and the time it required. One example for a final output looks like this:
....
Function                                 Time Taken
run_setup                                00:00:53
run_training_data                        00:21:36
run_validation_data                      00:21:12
run_prediction_data                      00:47:26
collect_similarity_comparison_data       00:00:00
calculate_similarities                   00:00:08

Total execution time                     01:31:15
....

.Steps the script executes:
[%collapsible]
====
. Navigate to the Automations module:
+
[source,sh]
----
cd <RAST directory>/Automations
----
. Set up TeaStore on your local machine:
+
[source,sh]
----
cd Setup_TeaStore
./setup.sh
----
. Setup Python virtual environments:
+
[source,sh]
----
cd Setup_Python
./setup.sh
----
. Create Training data for the predictive model component:
+
[source,sh]
----
cd Training_Data
./launch_all.sh
----
+
Wait for the script to finish.
+
** The log file will be downloaded automatically in the folder `Training_Data/Kieker_logs_<timestamp>`.
** Create predictive models:
+
[source,sh]
----
./create_predictive_model.sh
----
+
** Wait for the script to finish. You will find the predictive models in the folder `Predictive_Models`.
** Copy the resulting models to the Simulator component:
+
[source,sh]
----
./copy_models_to_simulator.sh
----
+
. Create Validation data for Similarity Comparison:
+
[source,sh]
----
cd Validation_Data
./launch_teastore_loadtest.sh
----
+
Wait for the script to finish.
+
** The log files will be downloaded automatically in the folder `Validation_Data/Kieker_logs_<timestamp>`.
** The folder should contain four different log files with the `.dat` file extension.
** You need to create a database for each log file individually:
+
[source,sh]
----
./create_validation_databases.sh
----
+
** When you are done, you should have four databases located in the folder `Validation_Data/Databases`, one for each load intensity profile.
. Create Prediction data for Similarity Comparison:
+
[source,sh]
----
cd Prediction_Data
./launch_all.sh
----
+
After the script finishes, the resulting log files are located in subfolders in the folder `Prediction_Data`. The subfolders are named after the predictive model that the simulator used and each one has an additional subfolder for the value of `corr_max`.

====

==== Manual (deprecated, use automated workflow described above)

.Preparations
[%collapsible]
====

* Set up TeaStore according to the https://github.com/jtpgames/RAST/blob/main/docs/TeaStore/Deployment.adoc#setup-teastore[instructions].
* Clone this repository. Make sure to pull all git submodules as well:
+
[source,sh]
----
git clone https://github.com/jtpgames/RAST.git
cd RAST
./pull_all_submodules.sh
----

====

.Instructions
[%collapsible]
====

. Open your terminal and use a terminal multiplexer such as tmux to create four sessions. We will refer to these sessions by numbers:
    * Session (1): This session will be used to start the TeaStore or the Simulator. Navigate to the respective folder within the cloned repositories.
    * Session (2): This session will be used to start the Load Test. Navigate to the `locust_scripts` folder.
    * Session (3): This session will be used to make code changes to the `offical_teastore_locustfile.py` file, allowing you to modify the load intensity profile. 
      Navigate to the `locust_scripts/locust` folder and open the file using a text editor of your choice (e.g., Vim or Emacs).
    * Session (4): This session will be used to make code changes to the `teastore.kt` file, enabling you to modify the predictive model. 
      Navigate to the Simulators folder and open the file.
. In Session (1), start the TeaStore or the Simulator based on the measurements you wish to acquire.
   For the purpose of this explanation, we will focus on starting the Simulator. 
   Navigate to your local Simulator folder and execute the command `./gradlew run`. 
   If successful, you will see the following line printed on the console: `INFO ktor.application - Responding at http://0.0.0.0:8081`. 
   To terminate the Simulator, press `Ctrl + C`.
. In Session (2):
..  (Recommended):
...     Create a python virtual environment in a directory called `venv`, e.g., `python3 -m venv venv`
...     Run the command `source activate_venv.sh` to activate the Python virtual environment (venv).
...     Run `pip install -r requirements.txt`
..  Execute `./start_teastore_loadtest.sh` to initiate the load test.
    This repository uses a low load intensity by default.
    The load test will automatically conclude after approximately two minutes.
..  Clean the folder by executing `./delete_results.sh`.
. In Session (4), you can now examine the `teastore_simulation.log` file.
  This file contains simulated processing times generated by the predictive model, among other relevant information.
. To modify the load intensity profile,
  navigate to Session (3) and locate the `StagesShape` class within the `offical_teastore_locustfile.py` file.
  Look for the line `load_intensity_profile: LoadIntensityProfile = LoadIntensityProfile.LOW`.
  Set `load_intensity_profile` to your desired value.
. To modify the predictive model,
  navigate to Session (4) and follow the instructions in the README.md file within the Simulators repository.

====

[#_similarity_comparison]
=== Similarity Comparison
==== Workflow

To perform a similarity comparison, two types of datasets are required: prediction data and validation data.

*Prediction Data:* This consists of log files generated by the Simulator.

*Validation Data:* This consists of SQLite databases created by extracting request logs from TeaStore.

Both the Simulator and TeaStore undergo identical load tests to ensure consistency in the comparison.

After acquiring the datasets, the `ResultComparer` is used to determine their similarity. The `ResultComparer` generates a `similarity_scores.csv` file. To facilitate easier analysis of this data, we provide an *.ods file where you can import the contents. This file constructs the figures published in our paper. Follow these steps to import the *.csv file (assuming you have followed our automated workflow to acquire the `similarity_scores.csv` file):

. Open the `Similarity_Comparison/template_similarity_scores.ods` file <<step_1>>.
. Open the `Similarity_Comparison/similarity_scores.csv` file and copy all its contents to the clipboard (Ctrl + A, Ctrl + C) <<step_2>>.
. Open the `InputFromCsv` sheet in the .ods file, select columns A to E, and paste the contents from the clipboard (Ctrl + V). The Text Import dialog will open. Press OK to complete the import <<step_3>>.
. You can find the figures on the `similarity_scores` sheet.

:imagesdir: ../Images/Similarity Comparison

.Template and result file for similarity comparison
[#step_1]
image::Screen_01.png[width=500]

.Example contents of similarity_scores.csv file
[#step_2]
image::Screen_02.png[width=500]

.Text Import into similarity_scores.ods file
[#step_3]
image::Screen_03.png[width=800]


[bibliography]
== References

* [[[teastore_fork]]](https://github.com/jtpgames/TeaStore)
* [[[simulator_repo]]](https://github.com/jtpgames/Simulators)
* [[[datalore_notebook]]](https://datalore.jetbrains.com/notebook/6K6VkECuLMtN5t5nSYg6WK/TVGp1egwDQlwI19astdVlM)

